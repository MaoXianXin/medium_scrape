"""
æ–‡æ¡£çš„å¤„ç†æµç¨‹ä¸º: æ–‡æ¡£åŠ è½½->æ–‡æ¡£åˆ†å‰²->åµŒå…¥->å‘é‡å­˜å‚¨->æ£€ç´¢->é—®ç­”

graph TD
    A[æ–‡æ¡£å¤„ç†å¼€å§‹] --> B[åˆå§‹åŒ– DocumentProcessor]
    
    subgraph åˆå§‹åŒ–é˜¶æ®µ
    B --> C[è®¾ç½® OpenAI API Key]
    B --> D[åˆå§‹åŒ–ç»„ä»¶]
    D -->|1| E[OpenAIEmbeddings<br>model: text-embedding-3-large<br>base_url: å¯é€‰]
    D -->|2| F[RecursiveCharacterTextSplitter<br>chunk_size: 1000<br>chunk_overlap: 200<br>add_start_index: true]
    D -->|3| G[InMemoryVectorStore]
    D -->|4| Q[ChatOpenAI<br>model: claude-3-5-sonnet<br>temperature: 0<br>base_url: å¯é€‰]
    E --> G
    end
    
    subgraph æ–‡æ¡£å¤„ç†æµç¨‹
    H[process_documentæ–¹æ³•] --> I[åŠ è½½æ–‡æ¡£<br>load_document]
    I --> J[æ–‡æ¡£åˆ†å‰²<br>split_documents]
    J --> K[æ·»åŠ åˆ°å‘é‡å­˜å‚¨<br>add_to_vectorstore]
    end
    
    subgraph æœç´¢åŠŸèƒ½
    L[æœç´¢æ–¹æ³•] --> M[ç›¸ä¼¼åº¦æœç´¢<br>similarity_search<br>å‚æ•°: k=4]
    L --> N[MMRæœç´¢<br>mmr_search<br>å‚æ•°: k=4, fetch_k=20]
    L --> O[ç›¸ä¼¼åº¦é˜ˆå€¼æœç´¢<br>similarity_score_threshold_search<br>å‚æ•°: score_threshold=0.8]
    M --> P[è¿”å›ç›¸å…³æ–‡æ¡£]
    N --> P
    O --> P
    end

    subgraph é—®ç­”åŠŸèƒ½
    R[é—®ç­”æµç¨‹] --> S[ç”¨æˆ·æŸ¥è¯¢]
    S --> M
    P --> T[åˆå¹¶æ–‡æ¡£å†…å®¹]
    T --> U[æ„å»ºç³»ç»Ÿæç¤ºè¯]
    U --> V[ç”Ÿæˆå›ç­”<br>ChatOpenAI.invoke]
    Q --> V
    end

    %% æ·»åŠ å­å›¾ä¹‹é—´çš„å…³ç³»
    F --> J
    G --> K
    
    E -.->|æŸ¥è¯¢å‘é‡åŒ–| M
    E -.->|æŸ¥è¯¢å‘é‡åŒ–| N
    E -.->|æŸ¥è¯¢å‘é‡åŒ–| O
    G -.->|å‘é‡å­˜å‚¨æ£€ç´¢| M
    G -.->|å‘é‡å­˜å‚¨æ£€ç´¢| N
    G -.->|å‘é‡å­˜å‚¨æ£€ç´¢| O

    %% æ·»åŠ æµç¨‹é¡ºåº
    åˆå§‹åŒ–é˜¶æ®µ --> æ–‡æ¡£å¤„ç†æµç¨‹
    æ–‡æ¡£å¤„ç†æµç¨‹ --> æœç´¢åŠŸèƒ½
    æœç´¢åŠŸèƒ½ --> é—®ç­”åŠŸèƒ½

    %% æ·»åŠ è¯´æ˜
    style åˆå§‹åŒ–é˜¶æ®µ fill:#f9f,stroke:#333,stroke-width:2px
    style æ–‡æ¡£å¤„ç†æµç¨‹ fill:#bbf,stroke:#333,stroke-width:2px
    style æœç´¢åŠŸèƒ½ fill:#bfb,stroke:#333,stroke-width:2px
    style é—®ç­”åŠŸèƒ½ fill:#fbb,stroke:#333,stroke-width:2px
"""

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import os
from langchain_openai.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
import hashlib
class DocumentProcessor:
    def __init__(self, openai_api_key, base_url=None):
        # åˆå§‹åŒ–é…ç½®
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            base_url=base_url if base_url else "https://api.openai.com/v1"
        )
        # æ·»åŠ çˆ¶å—åˆ†å‰²å™¨
        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            add_start_index=True
        )
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=100,
            chunk_overlap=20,
            add_start_index=True
        )
        self.vector_store = Chroma(
            embedding_function=self.embeddings,
            persist_directory="./chroma_db",  # æŒ‡å®šå­˜å‚¨ç›®å½•
            collection_name="my_collection"    # å¯é€‰ï¼šæŒ‡å®šé›†åˆåç§°
        )
        self.llm = ChatOpenAI(
            model="claude-3-5-sonnet-20240620",
            temperature=0,
            openai_api_key=openai_api_key,
            base_url=base_url if base_url else "https://api.openai.com/v1"
        )
        
    def load_document(self, file_path):
        """åŠ è½½æ–‡æ¡£"""
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
        for doc in docs:
            doc.metadata['source'] = file_path
            doc.metadata['page'] = doc.metadata.get('page', 0)
        return docs
        
    @staticmethod
    def get_content_hash(text):
        """æ ¹æ®æ–‡æœ¬å†…å®¹ç”Ÿæˆå“ˆå¸Œå€¼"""
        return hashlib.md5(text.encode()).hexdigest()[:12]

    def split_documents(self, documents):
        """åˆ†å‰²æ–‡æ¡£ä¸ºçˆ¶å—å’Œå­å—"""
        # é¦–å…ˆåˆ›å»ºçˆ¶å—
        parent_chunks = self.parent_splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªçˆ¶å—åˆ›å»ºå­å—
        all_children = []
        parent_to_children = {}
        
        for parent in parent_chunks:
            # ä½¿ç”¨å†…å®¹å“ˆå¸Œä½œä¸ºçˆ¶å—ID
            parent_hash = self.get_content_hash(parent.page_content)
            parent.metadata['chunk_id'] = f'parent_{parent_hash}'
            
            # ä»çˆ¶å—åˆ›å»ºå­å—
            children = self.child_splitter.split_documents([parent])
            
            # ä¸ºå­å—æ·»åŠ çˆ¶å—å¼•ç”¨
            for i, child in enumerate(children):
                child_hash = self.get_content_hash(child.page_content)
                child.metadata['parent_id'] = parent.metadata['chunk_id']
                child.metadata['chunk_id'] = f'child_{child_hash}_{i}'
                all_children.append(child)
            
            # è®°å½•çˆ¶å—åˆ°å­å—çš„æ˜ å°„
            parent_to_children[parent.metadata['chunk_id']] = children
        
        self.parent_chunks = parent_chunks
        self.parent_to_children = parent_to_children
        
        return all_children
        
    def add_to_vectorstore(self, documents):
        """å°†å­å—æ·»åŠ åˆ°å‘é‡å­˜å‚¨"""
        return self.vector_store.add_documents(documents=documents, ids=[doc.metadata['chunk_id'] for doc in documents])

    def get_parent_chunk(self, child_doc):
        """æ ¹æ®å­å—è·å–å¯¹åº”çš„çˆ¶å—"""
        parent_id = child_doc.metadata.get('parent_id')
        if parent_id:
            for parent in self.parent_chunks:
                if parent.metadata['chunk_id'] == parent_id:
                    return parent
        return child_doc

    def get_formatted_source_info(self, doc):
        """è·å–æ ¼å¼åŒ–çš„æºæ–‡ä»¶ä¿¡æ¯"""
        source = doc.metadata.get('source', 'Unknown source')
        page = doc.metadata.get('page', 'Unknown page')
        chunk_id = doc.metadata.get('chunk_id', 'Unknown chunk')
        return {
            'source': source,
            'page': page,
            'chunk_id': chunk_id
        }

    def similarity_search(self, query, k=4):
        """ç›¸ä¼¼åº¦æœç´¢ï¼Œè¿”å›çˆ¶å—åŠå…¶æºä¿¡æ¯"""
        child_results = self.vector_store.similarity_search(query, k=k)
        # ä½¿ç”¨é›†åˆæ¥è·Ÿè¸ªå·²ç»æ·»åŠ çš„çˆ¶å—ID
        seen_parent_ids = set()
        results = []
        for doc in child_results:
            parent_doc = self.get_parent_chunk(doc)
            parent_id = parent_doc.metadata['chunk_id']
            # åªæœ‰å½“çˆ¶å—IDæœªè¢«å¤„ç†è¿‡æ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
            if parent_id not in seen_parent_ids:
                seen_parent_ids.add(parent_id)
                results.append({
                    'document': parent_doc,
                    'source_info': self.get_formatted_source_info(parent_doc)
                })
        return results
        
    def mmr_search(self, query, k=4, fetch_k=20):
        """æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼Œè¿”å›çˆ¶å—åŠå…¶æºä¿¡æ¯"""
        child_results = self.vector_store.max_marginal_relevance_search(
            query,
            k=k,
            fetch_k=fetch_k
        )
        # ä½¿ç”¨é›†åˆæ¥è·Ÿè¸ªå·²ç»æ·»åŠ çš„çˆ¶å—ID
        seen_parent_ids = set()
        results = []
        for doc in child_results:
            parent_doc = self.get_parent_chunk(doc)
            parent_id = parent_doc.metadata['chunk_id']
            # åªæœ‰å½“çˆ¶å—IDæœªè¢«å¤„ç†è¿‡æ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
            if parent_id not in seen_parent_ids:
                seen_parent_ids.add(parent_id)
                results.append({
                    'document': parent_doc,
                    'source_info': self.get_formatted_source_info(parent_doc)
                })
        return results
        
    def similarity_score_threshold_search(self, query, score_threshold=0.8):
        """ç›¸ä¼¼åº¦é˜ˆå€¼æœç´¢ï¼Œè¿”å›çˆ¶å—åŠå…¶æºä¿¡æ¯"""
        results = self.vector_store.similarity_search_with_score(query)
        # ä½¿ç”¨é›†åˆæ¥è·Ÿè¸ªå·²ç»æ·»åŠ çš„çˆ¶å—ID
        seen_parent_ids = set()
        filtered_results = []
        for doc, score in results:
            if score >= score_threshold:
                parent_doc = self.get_parent_chunk(doc)
                parent_id = parent_doc.metadata['chunk_id']
                # åªæœ‰å½“çˆ¶å—IDæœªè¢«å¤„ç†è¿‡æ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
                if parent_id not in seen_parent_ids:
                    seen_parent_ids.add(parent_id)
                    filtered_results.append({
                        'document': parent_doc,
                        'source_info': self.get_formatted_source_info(parent_doc),
                        'score': score
                    })
        return filtered_results

    def process_document(self, file_path):
        """å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹"""
        # 1. åŠ è½½æ–‡æ¡£
        docs = self.load_document(file_path)
        
        # 2. åˆ†å‰²æ–‡æ¡£
        splits = self.split_documents(docs)
        
        # 3. æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.add_to_vectorstore(splits)
        
        return "Document processed successfully"

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆæ·»åŠ å¯é€‰çš„ base_url å‚æ•°ï¼‰
    processor = DocumentProcessor(
        "sk-noerGmiAt3J8SQdnj1UI74K4ixZhB55OUuEp6rfa85BOjVcI",
        base_url="https://zzzzapi.com/v1"  # å¯é€‰å‚æ•°
    )
    
    # å¤„ç†æ–‡æ¡£
    file_path = "/home/mao/Downloads/Introduction _ ğŸ¦œï¸ğŸ”— LangChain.pdf"
    print("å¼€å§‹å¤„ç†æ–‡æ¡£...")
    result = processor.process_document(file_path)
    print(result)
    
    # ç”¨æˆ·æŸ¥è¯¢
    query = "What is LangSmith?"
    print("\nç”¨æˆ·æŸ¥è¯¢:", query)
    
    # ç›¸ä¼¼åº¦æœç´¢è·å–ç›¸å…³æ–‡æ¡£
    similar_docs = processor.similarity_search(query)
    
    # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆå¹¶ä¸ºæ–‡æœ¬
    docs_text = "\n".join(doc['document'].page_content for doc in similar_docs)
    
    # å®šä¹‰ç³»ç»Ÿæç¤ºè¯
    system_prompt = """You are an assistant for question-answering tasks. 
    Use the following pieces of retrieved context to answer the question. 
    If you don't know the answer, just say that you don't know. 
    Keep the answer clear and concise while providing complete information.
    Context: {context}"""
    
    system_prompt_fmt = system_prompt.format(context=docs_text)
    
    # ç”Ÿæˆå›ç­”
    messages = [
        SystemMessage(content=system_prompt_fmt),
        HumanMessage(content=query)
    ]
    response = processor.llm.invoke(messages)
    
    print("\næ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
    for i, doc in enumerate(similar_docs, 1):
        print(f"\næ–‡æ¡£ {i}:")
        print(f"Source: {doc['source_info']['source']}, Page: {doc['source_info']['page']}, Chunk ID: {doc['source_info']['chunk_id']}")
        print(doc['document'].page_content[:200] + "...")
    
    print("\nAI å›ç­”:")
    print(response.content)

if __name__ == "__main__":
    main()