"""
æ–‡æ¡£çš„å¤„ç†æµç¨‹ä¸º: æ–‡æ¡£åŠ è½½->æ–‡æ¡£åˆ†å‰²->åµŒå…¥->å‘é‡å­˜å‚¨

graph TD
    A[å¼€å§‹] --> B[åˆå§‹åŒ– DocumentPipeline]
    B --> C{é…ç½®åˆå§‹åŒ–}
    C --> |1| D[è®¾ç½® OpenAI API]
    C --> |2| E[åˆå§‹åŒ– Embeddings æ¨¡å‹]
    C --> |3| F[åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨]
    C --> |4| G[åˆå§‹åŒ–å‘é‡å­˜å‚¨]
    C --> |5| G2[åˆå§‹åŒ–å­˜å‚¨å®¹å™¨<br>parent_chunks<br>parent_to_children]
    
    H[process_document] --> I[åŠ è½½æ–‡æ¡£<br>load_document]
    I --> J[PDFåŠ è½½å™¨å¤„ç†]
    J --> K[æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯]
    
    K --> L[åˆ†å‰²æ–‡æ¡£<br>split_documents]
    L --> M[åˆ›å»ºçˆ¶å—]
    M --> N[ä¸ºæ¯ä¸ªçˆ¶å—åˆ›å»ºå­å—]
    N --> O[ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼]
    O --> P[å»ºç«‹çˆ¶å­å—å…³ç³»]
    
    P --> P2[å­˜å‚¨å¤„ç†ç»“æœ]
    P2 --> |å­˜å‚¨çˆ¶å—| P3[æ›´æ–° parent_chunks]
    P2 --> |å­˜å‚¨æ˜ å°„å…³ç³»| P4[æ›´æ–° parent_to_children]
    P2 --> |è¿”å›æ‰€æœ‰å­å—| P5[ç”Ÿæˆ splits]
    
    P5 --> Q[å‘é‡å­˜å‚¨å¤„ç†]
    Q --> R[æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨]
    
    R --> S[è¿”å›å¤„ç†ç»“æœ]
    S --> S1[è¿”å›çŠ¶æ€: success]
    S --> S2[è¿”å›çˆ¶å—æ•°é‡]
    S --> S3[è¿”å›å­å—æ•°é‡]
    S --> T[ç»“æŸ]
    
    subgraph æ–‡æœ¬åˆ†å‰²é…ç½®
    F1[çˆ¶å—åˆ†å‰²å™¨<br>chunk_size=1000<br>overlap=200]
    F2[å­å—åˆ†å‰²å™¨<br>chunk_size=100<br>overlap=20]
    end
    
    subgraph å…ƒæ•°æ®å¤„ç†
    O1[çˆ¶å—ID: parent_hash]
    O2[å­å—ID: child_hash]
    O3[å…³è”çˆ¶å­å—ID]
    end
"""

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import os
import hashlib

class DocumentPipeline:
    def __init__(self, openai_api_key, base_url=None):
        # åˆå§‹åŒ–é…ç½®
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            base_url=base_url if base_url else "https://api.openai.com/v1"
        )
        
        # åˆå§‹åŒ–åˆ†å‰²å™¨
        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            add_start_index=True
        )
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=100,
            chunk_overlap=20,
            add_start_index=True
        )
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = Chroma(
            embedding_function=self.embeddings,
            persist_directory="./chroma_db",  # æŒ‡å®šå­˜å‚¨ç›®å½•
            collection_name="my_collection"    # å¯é€‰ï¼šæŒ‡å®šé›†åˆåç§°
        )
        
        # å­˜å‚¨æ–‡æ¡£å—çš„å¼•ç”¨
        self.parent_chunks = []
        self.parent_to_children = {}

    def load_document(self, file_path):
        """åŠ è½½PDFæ–‡æ¡£"""
        loader = PyPDFLoader(file_path)
        docs = loader.load()
        # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
        for doc in docs:
            doc.metadata['source'] = file_path
            doc.metadata['page'] = doc.metadata.get('page', 0)
        return docs

    @staticmethod
    def get_content_hash(text):
        """ç”Ÿæˆæ–‡æœ¬å†…å®¹çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(text.encode()).hexdigest()[:12]

    def split_documents(self, documents):
        """å°†æ–‡æ¡£åˆ†å‰²ä¸ºçˆ¶å—å’Œå­å—"""
        # åˆ›å»ºçˆ¶å—
        parent_chunks = self.parent_splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªçˆ¶å—åˆ›å»ºå­å—
        all_children = []
        parent_to_children = {}
        
        for parent in parent_chunks:
            parent_hash = self.get_content_hash(parent.page_content)
            parent.metadata['chunk_id'] = f'parent_{parent_hash}'
            
            children = self.child_splitter.split_documents([parent])
            
            for child in children:
                child_hash = self.get_content_hash(child.page_content)
                child.metadata['parent_id'] = parent.metadata['chunk_id']
                child.metadata['chunk_id'] = f'child_{child_hash}'
                all_children.append(child)
            
            parent_to_children[parent.metadata['chunk_id']] = children
        
        self.parent_chunks = parent_chunks
        self.parent_to_children = parent_to_children
        
        return all_children

    def process_document(self, file_path):
        """æ‰§è¡Œå®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹"""
        # 1. åŠ è½½æ–‡æ¡£
        docs = self.load_document(file_path)
        
        # 2. åˆ†å‰²æ–‡æ¡£
        splits = self.split_documents(docs)
        
        # 3. æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vector_store.add_documents(documents=splits, ids=[doc.metadata['chunk_id'] for doc in splits])
        
        return {
            "status": "success",
            "parent_chunks_count": len(self.parent_chunks),
            "child_chunks_count": len(splits)
        }

    def get_collection_stats(self) -> dict:
        """è·å–å‘é‡æ•°æ®åº“é›†åˆçš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict: åŒ…å«ä»¥ä¸‹ç»Ÿè®¡ä¿¡æ¯:
                - total_documents: æ–‡æ¡£æ€»æ•°
                - unique_sources: ä¸åŒæºæ–‡ä»¶æ•°é‡åŠæ¸…å•
        """
        # è·å–æ‰€æœ‰æ–‡æ¡£
        results = self.vector_store.get()
        
        # ç»Ÿè®¡æ€»æ–‡æ¡£æ•°
        total_docs = len(results["ids"]) if "ids" in results else 0
        
        # ç»Ÿè®¡ä¸åŒæºæ–‡ä»¶
        unique_sources = set()
        if "metadatas" in results:
            for metadata in results["metadatas"]:
                if metadata and "source" in metadata:
                    unique_sources.add(metadata["source"])
        
        return {
            "total_documents": total_docs,
            "unique_sources": {
                "count": len(unique_sources),
                "sources": list(unique_sources)
            }
        }
    
    def reset_collection(self):
        """æ¸…ç©ºå‘é‡æ•°æ®åº“é›†åˆå¹¶é‡ç½®çŠ¶æ€"""
        # é‡ç½®å‘é‡å­˜å‚¨
        self.vector_store.reset_collection()
        
        # é‡ç½®å†…éƒ¨çŠ¶æ€
        self.parent_chunks = []
        self.parent_to_children = {}

# ä½¿ç”¨ç¤ºä¾‹
def main():
    pipeline = DocumentPipeline(
        openai_api_key="sk-noerGmiAt3J8SQdnj1UI74K4ixZhB55OUuEp6rfa85BOjVcI",
        base_url="https://zzzzapi.com/v1"  # å¯é€‰
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = pipeline.get_collection_stats()
    print(f"æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
    print(f"æºæ–‡ä»¶æ•°é‡: {stats['unique_sources']['count']}")
    print(f"æºæ–‡ä»¶åˆ—è¡¨: {stats['unique_sources']['sources']}")

    # é‡ç½®é›†åˆ
    # pipeline.reset_collection()

    result = pipeline.process_document("/home/mao/Downloads/Introduction _ ğŸ¦œï¸ğŸ”— LangChain.pdf")
    print(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼š{result}")

if __name__ == "__main__":
    main()