给定一本书《置身时代的社会理论》，这本书有387页，总计407296个tokens
现在按照每7000个tokens进行分割，得到59个段落，每个段落经过文章总结提示词，得到文章总结，然后摘出来核心观点部分内容
把所有核心观点写入一个txt中，总计43958个tokens，那么我们实现了10倍左右的信息压缩

之后利用GPT在压缩后的文本上做挖掘，首先需要建立知识地图，把握知识脉络，在这个过程中对于感兴趣的知识点，我们可以让GPT展开讲讲